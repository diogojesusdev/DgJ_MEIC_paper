\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\usepackage{listings}
\lstset{
  xleftmargin=2em,       % add left margin inside the text area
  framexleftmargin=1.5em, % adjust frame (if any)
  numbers=left,
  numbersep=10pt
}
\usepackage{caption}

\usepackage{algorithm}
\usepackage{algpseudocode}

\captionsetup[lstlisting]{position=bottom}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Suppress overfull/underfull \hbox warnings
\hfuzz=10pt        % allows overfull hboxes up to 10pt without warning
\hbadness=10000    % suppresses underfull hbox warnings
\vbadness=10000    % suppresses underfull vbox warnings

\begin{document}

\title{Serverless Dataflows: A Decentralized Workflow Execution Engine with Predictive Planning}
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}

% \author{\IEEEauthorblockN{1\textsuperscript{st} Diogo Jesus}
\author{\IEEEauthorblockN{Diogo Jesus}
% \IEEEauthorblockA{\textit{dept. name of organisation (of Aff.)} \\
\IEEEauthorblockA{
\textit{Instituto Superior Tecnico (IST), INESC-ID Lisboa}\\
Lisbon, Portugal \\
diogofjesus@inesc-id.pt}}
\maketitle

\begin{abstract}
Serverless computing has become a suitable cloud paradigm for many applications, prized for its operational ease, automatic scalability, and fine-grained pay-per-use pricing model. However, executing workflows, which are compositions of multiple tasks, in Function-as-a-Service (FaaS) environments remains inefficient. This inefficiency arises from the stateless nature of functions, which forces excessive use of external services for intermediate data handling and inter-function communication.

In this document, we introduce an adaptive decentralized DAG engine that leverages historical metadata to plan and influence task scheduling. Our solution encompasses metadata management, static workflow planning, and a worker-level scheduling strategy designed to drive workflow execution with minimal synchronization. We compare our system against WUKONG, a state-of-the-art decentralized serverless DAG engine. Our evaluation shows that our most resource-efficient planner reduces workflow execution time by \textbf{12.6\%} and simultaneously lowers resource consumption by \textbf{36\%} compared to the optimized WUKONG scheduling approach. For users prioritizing speed over cost, our fastest planner (Non-Uniform optimized) achieves a \textbf{57.5\%} reduction in makespan compared to our most resource-efficient planner (Uniform), but this performance gain requires a \textbf{114\%} increase in resource consumption.
\end{abstract}

\begin{IEEEkeywords}
Cloud Computing, Serverless, FaaS, Serverless Workflows, DAG, Metadata, Workflow Prediction
\end{IEEEkeywords}

\section{Introduction}
\label{s:introduction}

Function-as-a-Service (FaaS) represents a serverless cloud computing paradigm that simplifies application deployment by abstracting away infrastructure management. It provides automatic, elastic scalability (potentially without limit) along with a fine-grained, pay-per-use pricing model. This has led to its widespread adoption for event-driven systems, microservices, and web services on platforms like AWS Lambda\cite{aws_lambda}, Azure Functions\cite{azure_functions}, and Google Cloud Functions\cite{google_cloud_run_functions}. These applications typically benefit the most from FaaS because they are lightweight, stateless, and characterized by highly variable or unpredictable workloads, allowing them to leverage serverless platforms' on-demand scalability and cost-efficiency.

This paradigm is also increasingly used to execute complex scientific and data processing workflows, such as the Cybershake~\cite{cybershake_workflow} seismic hazard analysis or Montage~\cite{montage_astronomy}, an astronomy image mosaicking workflow. These applications are structured as workflows, usually represented as Directed Acyclic Graphs (DAGs) of interdependent tasks. However, efficiently executing these complex workflows on serverless platforms remains a significant challenge. 

Despite their advantages, serverless platforms present several limitations that complicate the execution of complex workflows. Since these platforms allow scaling down to zero resources to save costs, they can also introduce unpredictable latency, known as \textit{cold starts}~\cite{cold_starts_surey}, particularly for short-lived functions, affecting overall workflow performance. The lack of \textit{direct inter-function communication}~\cite{serverless_computing_drawbacks_survey_rw1} means that tasks often have to rely on external services, such as message brokers or databases to exchange intermediate data, which can increase overhead and reduce efficiency. Interoperability between platforms is further limited by the use of platform-specific workflow definition languages, which restricts the portability of workflows across different serverless environments. Additionally, while statelessness simplifies scaling and management, it can introduce overhead and complexity for applications that require continuity or coordination across multiple function invocations. Finally, developers have limited control over the underlying infrastructure, restricting the ability to optimize resource usage or tune performance for specific workloads.

Several solutions have emerged to address the limitations of serverless platforms. Stateful functions (e.g., AWS Step Functions\cite{aws_step_functions}, Azure Durable Functions\cite{azure_durable_functions}, and Google Cloud Workflows\cite{google_cloud_workflows}) expand the range of applications that can run on serverless platforms by maintaining state across multiple function invocations, coordinating complex workflows, and providing built-in fault tolerance. Other approaches tackle limitations at the runtime level, proposing extensions to FaaS platforms (e.g., Faa\$T~\cite{faast_caching}, Palette~\cite{palette_load_balancing}, Lambdata~\cite{lambdata_intents}) or entirely new serverless architectures (e.g., Apache OpenWhisk~\cite{open_whisk}). Finally, some workflow-focused solutions (e.g., WUKONG~\cite{wukong_2}, Unum~\cite{unum_decentralized_orchestrator}, DEWEv3~\cite{dewe_v3}) employ scheduling strategies and workflow-level optimizations on unmodified FaaS platforms to enhance efficiency, primarily by improving \textit{data locality}, which brings computation closer to the data and minimizes reliance on external services for data exchange and synchronization.


These workflow-focused approaches, however, often use uniform resources for workers and rely on \textit{“one-step scheduling”}, making decisions based solely on the immediate workflow stage without considering the broader context or the downstream effects of their decisions. Other solutions often use homogeneous worker configurations, which can lead to an inefficient use of resources when tasks have diverse computational or memory requirements. Furthermore, existing heuristic-driven approaches can be inefficient in many scenarios because they provide no mechanism for tailoring worker resource allocations to the specific requirements of individual tasks. This often leads to a suboptimal trade-off between data locality and resource contention. Moreover, we found no prior work that leverages metadata or historical metrics to inform scheduling decisions across an entire serverless workflow.  

This limitation motivates the central research question of this work: if we have knowledge of the computation steps, collect sufficient metrics on their behavior, and understand how they are composed to form the full workflow, can we make smarter scheduling decisions that \textbf{minimize makespan} and \textbf{maximize resource efficiency} in a FaaS environment?  

To answer this research question, we propose an adaptive decentralized serverless workflow execution engine that leverages historical metadata from previous workflow runs to generate informed task allocation plans, which are then executed by FaaS workers in a choreographed manner, without needing a central scheduler. By relying on such planning, our approach aims to minimize the usage of external cloud storage services, which are often employed by similar solutions for intermediate data exchange and synchronization, while also avoiding the inefficiencies of homogeneous worker resource allocations.

The main contributions of this work are as follows:
\begin{itemize}
    \item Analysis of the serverless workflow orchestration landscape;
    \item Propose an adaptive decentralized serverless workflow execution engine that overcomes the "one-step scheduling" and uniform-resource limitations of existing workflow-focused solutions by leveraging historical metadata to generate informed execution plans;
    \item Demonstrate how incorporating historical execution data can be used to improve task allocation (balancing locality and resource contention), reducing usage of external cloud storage services and enhancing overall workflow efficiency on FaaS platforms.
\end{itemize}

% ------------------------------------------------ %

\section{Architecture}
\label{s:architecture}

As we have stated, most existing serverless schedulers employ an approach where decisions are made based solely on the immediate workflow stage without considering the global implications. We hereby propose a novel \textit{adaptive decentralized serverless workflow execution engine} that leverages historical metadata from previous workflow runs to make lightweight predictions and create workflow plans before they execute. Such plans include information about where to execute each task (locality), the worker resource configuration to use (how much vCPUs and Memory) and optimizations. At run-time, the workers will execute the plan and apply the specified optimizations. Our solution was written in \textit{Python}, a language known for its simplicity and popularity among data scientists.

\subsection{Workflow Definition Language}
\label{ss:workflow_definition_language}

We will now present our workflow engine that transforms ordinary Python functions into parallelizable tasks, automatically managing dependencies and execution through an intuitive decorator-based API. It is inspired by WUKONG, Dask and Airflow's way of expressing workflows: the user can create workflows by composing individual Python functions, as shown in Listing \ref{lst:dag_lang_example}. In this example, we define two tasks, \texttt{task\_a} and \texttt{task\_b}, and then compose them into a DAG by passing their results as arguments to the next task. The resulting workflow structure is illustrated in Figure \ref{fig:dag_lang_example}.

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, columns=fullflexible, breaklines=true, caption=DAG definition example, label=lst:dag_lang_example]
# 1) Task definition
@DAGTask
def task_a(a: int) -> int:
    # ... user code logic ...
    return a + 1

@DAGTask(forced_optimizations=[PreLoadOptimization()])
def task_b(*args: int) -> int:
    # ... user code logic ...
    return sum(args)

# 2) Task composition (DAG/Workflow)
a1 = task_a(10)
a2 = task_a(a1)
a3 = task_a(a1)
b1 = task_b(a2, a3)
a4 = task_a(b1)
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/dag_lang_example.png}
    \caption{Simple DAG example}
    \label{fig:dag_lang_example}
\end{figure}

It is important to note that while this example passes data directly, passing storage references (e.g., cloud object storage URLs) as function arguments is a common pattern in serverless workflows that is not supported by our solution. In future iterations, this limitation could be addressed through \textbf{code instrumentation}, wherein storage access APIs would be intercepted to record metrics such as the volume of data transferred and the corresponding I/O latency.

When \texttt{task\_a(10)} is invoked, it doesn't actually run the user code. It instead creates a representation of the task, which can be passed as argument to other tasks. The workflow planning and execution only happens once \texttt{.compute()} is called on the last/sink task (\texttt{a4}), as shown in Listing~\ref{lst:setup_and_launch_workflow_execution}. When \texttt{compute()} is called, a DAG representation of the entire workflow structure is created by backtracking the task dependencies.

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, columns=fullflexible, breaklines=true, caption=Setting up and launching workflow execution, label=lst:setup_and_launch_workflow_execution]
result = a4.compute(
    dag_name="simpledag", 
    config=Worker.Config(
        faas_gateway_address=...,
        intermediate_storage_config=(ip, port, password),
        metrics_storage_config=(ip, port, password),
        planner_config=UniformPlanner.Config(
            sla=sla,
            worker_resource_configuration=TaskWorkerResourceConfiguration(cpus=3, memory_mb=512),
            optimizations=[PreLoadOptimization, PreWarmOptimization]
        )
    )
)
\end{lstlisting}

One limitation of our DAG definition approach is that it doesn't support "dynamic fan-outs" (e.g., creating a variable number of tasks depending on the result of another task) on a single workflow. This is a powerful and expressive feature, but that is seldom supported in other DAG definition languages (e.g., Dask~\cite{dask_python}, WUKONG~\cite{wukong_2}, Unum~\cite{unum_decentralized_orchestrator}, Oozie~\cite{apache_oozie} do not support it). These languages require the user to split the workflow into multiple workflows, one for each \textit{dynamic fan-out}: one workflow runs up to the task that generates a list of results, while a second workflow starts with a number of tasks that depends on the size or contents of that list. 

Apache AirFlow~\footnote{https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html} supports this feature through an extension to their DAG language, allowing a variable number of tasks to be created at run-time depending on the number of results produced by a previous task. Implementing similar functionality is possible, but it would reduce the accuracy of predictions. This is because we would also need to predict the expected fan-out size, and any errors in that prediction could amplify inaccuracies in the predictions for the rest of the workflow.

We will now present our solution architecture overview, highlighting the core layers of our decentralized serverless workflow execution engine.

\subsection{Architecture Overview}

Figure~\ref{fig:solution_architecture} shows the overall architecture and logical flow of our decentralized serverless workflow execution engine, which is organized into 3 high-level layers. The upper part of the figure represents the components that run on the user's machine, while the lower part represents the components that run outside the user's machine.

After the user writes its workflow in Python, as demonstrated in Section \ref{ss:workflow_definition_language}, it can then specify a planning algorithm, which will run locally to generate a static workflow plan, defining a task-to-worker mapping and other task-level optimization hints for FaaS workers. Once the plan is created, the client launches the initial workers for the root tasks, kicking off workflow execution. The user program then waits for a storage notification indicating workflow completion, when it finally retrieves the result from storage.

The following sections should provide a deeper understanding of each layer as well as how the user interacts with the system.

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.95\textwidth]{figures/solution_distributedarchintegrated.png}
  \caption{Solution Architecture}
  \label{fig:solution_architecture}
\end{figure*}

\begin{enumerate}
    \item \textbf{Metadata Management}: Responsible for collecting and storing task metadata from previous executions. It then uses this metadata to provide predictions regarding task execution times, data transfer times, task output sizes, and worker startup times;
    \item \textbf{Static Workflow Planning}: Receives the entire workflow, represented as a Directed Acyclic Graph (DAG), and a "Planner" (an algorithm chosen by the user). This Planner will use the predictions provided by Metadata Management to create a static workflow plan/schedule;
    \item \textbf{Decentralized Scheduling}: This component is integrated into the workers, and it is responsible for executing the plan generated by the Static Workflow Planning layer, applying optimizations and delegating tasks as needed without the need for a central entity.
\end{enumerate}

There are 3 distinct computational entities involved in this system:

\begin{itemize}
    \item \textbf{User Computer}: Responsible for creating workflow plans, submitting them (triggering workflow execution), and receiving its results;
    \item \textbf{Workers}: These are the FaaS workers (often running in containerized environments), that execute one or more tasks of the workflow. The decentralization of our solution is due to the fact that these workers schedule subsequent tasks, delegate tasks and launch new workers when needed without requiring a central scheduling unit. Lastly, they are also responsible for collecting and uploading metadata;
    \item \textbf{Storage}: Consists of an \textit{Intermediate Storage} for task outputs which may be needed for subsequent tasks and a \textit{Metadata Storage} for storing metrics (used for predictions) and information crucial to workflow execution (e.g., notifications about task readiness and completion).
\end{itemize}

Next, we will go through the \textbf{three} layers that compose our solution: \textbf{Metadata Management}, \textbf{Static Workflow Planning}, and \textbf{Decentralized Scheduling}.

\subsection{Metadata Management}
\label{ss:metadata_management}
The goal of the \textbf{Metadata Management} layer is to provide accurate task-wise predictions to help the planner algorithm chosen by the user make better decisions. To achieve this, while the workflow is running, we collect metrics about each task. Some of these metrics are: task execution time, data transfer size and time, task input and output sizes, and worker startup time. 

Storing these metrics enables us to provide a prediction API, shown in Listing~\ref{lst:task_predictions_api}. To improve accuracy, metrics are kept separate for each workflow meaning that, even if two workflows use the same function or task code, their metrics are stored independently. This design choice reflects our assumption that different workflows have different characteristics and may have different execution patterns. Metrics are batched and uploaded when the worker shuts down, to reduce runtime overhead.

The prediction methods take an additional parameter, \texttt{SLA} (Service-level Agreement), which is specified by the user and influences the selection of prediction samples. For example, \texttt{SLA=Percentile(50)} will use the median of the historical samples, whereas \texttt{SLA=Percentile(80)} will return a more conservative estimate. By allowing the user to control this parameter, the API can provide predictions that are tailored to different performance requirements.

\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, columns=fullflexible, breaklines=true, caption=Task Predictions API, label=lst:task_predictions_api]
class PredictionsProvider:
    def predict_output_size(function_name, input_size, sla) -> int

    def predict_worker_startup_time(state: 'cold' | 'warm', resource_config, sla) -> float

    def predict_data_transfer_time(data_size_bytes, resource_config, sla) -> float

    def predict_execution_time(task_name, input_size, resource_config, sla) -> float
\end{lstlisting}
In addition, metrics such as worker startup time, data transfer time, and task execution time are tied to the specific worker resource configuration. To account for this, our prediction method follows two paths. If we have enough historical samples for the same resource configuration, we use only those. Otherwise, we fall back to a normalization strategy: we adjust samples from other memory configurations to a baseline, use those to estimate execution time, and then rescale the result back to the target configuration. 

After filtering samples, we use an algorithm that selects a limited number of the most relevant samples for each prediction. This algorithm works by gradually widening a tolerance window around the reference value until it finds enough nearby samples. Within each window, it balances samples that are smaller, larger, or exactly equal to the reference, giving preference to the closest ones. If there still aren't enough candidates, it falls back to simply picking the nearest available samples overall. This way, the algorithm adapts to the data while keeping the selection both relevant and limited in size.

\subsection{Static Workflow Planning}
\label{ss:static_workflow_planning}
This layer executes on the client, and it receives the workflow representation and a workflow planning algorithm chosen by the user (as shown in Listing \ref{lst:setup_and_launch_workflow_execution}). Its job is to run the planning algorithm, providing it access to the workflow DAG and predictions exposed by the Metadata Management layer (Section \ref{ss:metadata_management}).

Planners can run \textit{workflow simulations} based on the predictions, allowing them to experiment with different resource configurations for different tasks and different task co-location strategies. Additionally, they can apply different user-selected optimizations. The accuracy of these simulations depends on the accuracy of the predictions exposed by the \textit{Predictions API}.

For each task, the planner assigns both a \texttt{worker\_id} and a resource configuration (vCPUs and memory). The \texttt{worker\_id} specifies the worker instance that must execute the task, analogous to the “colors” in Palette Load Balancing~\cite{palette_load_balancing}, but in our case this assignment is mandatory rather than advisory, giving strict control over execution locality. Two tasks assigned the same \texttt{worker\_id} will be executed on the same worker instance. If \texttt{worker\_id} is not specified, workers will, at run-time, have to decide whether to execute or delegate those tasks, similar to WUKONG's ~\cite{wukong_2} scheduling. We refer to these workers as \textit{"flexible workers"}.

Users can select from three provided planners or implement their own planner by implementing an interface. All planners have access to the predictions API as well as the workflow simulation. The planners the user can choose from are the following:

\begin{enumerate}
    \item \textbf{WUKONG}: All tasks will use the same worker configuration (specified by the user) and won't be assigned a \texttt{worker\_id}, meaning they will be executed by \textit{"flexible workers"}. This is a dynamic scheduling approach where tasks aren't tied to specific workers, trying to reproduce WUKONG's scheduling behavior;
    \item \textbf{Uniform}: Tasks use a common worker configuration specified by the user, with each task assigned a \texttt{worker\_id}, allowing co-location of tasks;
    \item \textbf{Non-Uniform}: Tasks can use different worker configurations (a list of available resources is specified by the user). Each task is assigned a \texttt{worker\_id}. This algorithm starts by assigning the best available resources to all tasks. Then, it runs a resource downgrading algorithm that attempts to downgrade resources of workers \textit{outside the critical path} as much as possible without introducing a new critical path.
\end{enumerate}

\begin{algorithm}
    \caption{Worker Assignment Algorithm (used by Uniform and Non-Uniform planners)}
    \label{alg:worker_assignment}
    \small
    \begin{algorithmic}[1]
    \Require $nodes$, $predictions$, $MAX\_CLUSTERING$
    \State $assigned \gets \emptyset$
    
    \Comment{nodes are topologically sorted}
    \ForAll{$n \in nodes$}
        \If{$n \in assigned$}
            \State \textbf{continue}
        \EndIf
        
        \If{$n.upstream = \emptyset$} \Comment{root nodes}
            \State $roots \gets \{r \in nodes \mid r.upstream = \emptyset \land r \notin assigned\}$
            \State \Call{AssignGroup}{$\textit{null}$, $roots$}
        \ElsIf{$|n.upstream| = 1$} \Comment{1$\to$1 or 1$\to$N}
            \State $u \gets n.upstream[0]$
            \If{$|u.downstream| = 1$}
                \State \Call{AssignWorker}{$[n]$, $u.worker$} \Comment{reuse worker}
            \Else \Comment{1$\to$N}
                \State $fanout \gets \{d \in u.downstream \mid d \notin assigned\}$
                \State \Call{AssignGroup}{$u.worker$, $fanout$}
            \EndIf
        \Else \Comment{N$\to$1 (assign to worker of upstream task with the largest total output)}
            \State $outputs \gets \{u.worker: predictions.output\_size(u) \mid u \in n.upstream\}$
            \State $worker\_w\_greatest\_acc\_output \gets \arg\max_{w \in outputs} outputs[w]$
            \State \Call{AssignWorker}{$[n]$, $worker\_w\_greatest\_acc\_output$}
        \EndIf
    \EndFor
    \end{algorithmic}
\end{algorithm}

Both the \textbf{Uniform} and \textbf{Non-Uniform} planners follow a two-phase approach for task allocation: \textit{Resource Configuration assignment} followed by \textit{Worker ID assignment}. The planners differ in their resource allocation strategies. The \textbf{Uniform planner} applies a single, user-specified CPU and memory configuration to all tasks, while the \textbf{Non-Uniform planner} initially selects the most powerful configuration from the user-specified options for each task. After assigning initial resources, both planners employ the logic detailed in Algorithm \ref{alg:worker_assignment} (in Appendix) for Worker ID assignment. This algorithm implements a \textbf{balanced clustering strategy} that uses history to try maximizing data locality while avoiding resource contention. It achieves this by launching the minimal number of new workers necessary to maintain efficiency, while avoiding overloading any individual worker with excessive task assignments; and minimizing network data transfers by co-locating tasks whose outputs are expected to be larger. This clustering approach achieves a \textbf{balance between resource contention and data locality}, contrasting with \textbf{WUKONG}.

\begin{algorithm}
    \caption{AssignGroup Procedure}
    \label{alg:assign_group}
    \small
    \begin{algorithmic}[1]
    \Function{AssignGroup}{$up\_worker$, $tasks$}
        \If{$tasks = \emptyset$}
            \Return
        \EndIf
        
        \State $exec\_t \gets \{t: predictions.exec\_time(t) \mid t \in tasks\}$
        \State $out\_sz \gets \{t: predictions.output\_size(t) \mid t \in tasks\}$
        \State $median \gets \textsc{Median}(exec\_t.values())$
        \State $longs \gets \{t \in tasks \mid exec\_t[t] > median\}$
        \State $shorts \gets \textsc{SortLargerOutputFirst}(\{t \in tasks \mid exec\_t[t] \leq median\})$
        
        \Statex
        \State \Comment{1) short tasks with bigger outputs on upstream worker}
        \If{$up\_worker \neq \textit{null} \land shorts \neq \emptyset$}
            \State $cluster \gets shorts[0:MAX\_CLUSTERING]$
            \State \Call{AssignWorker}{$cluster$, $up\_worker$}
            \State $shorts \gets shorts[MAX\_CLUSTERING:]$
        \EndIf
        
        \Statex
        \State \Comment{2) pair long tasks with remaining shorts tasks}
        \While{$longs \neq \emptyset \land shorts \neq \emptyset$}
            \State $cluster \gets [longs[0]] + shorts[0:MAX\_CLUSTERING-1]$
            \State $worker\_id \gets$ \Call{NewWorkerId}{}
            \State \Call{AssignWorker}{$cluster$, $worker\_id$}
            \State $longs \gets longs[1:]$
            \State $shorts \gets shorts[MAX\_CLUSTERING-1:]$
        \EndWhile
        
        \Statex
        \State \Comment{3) group remaining short tasks}
        \While{$shorts \neq \emptyset$}
            \State $worker\_id \gets$ \Call{NewWorkerId}{}
            \State \Call{AssignWorker}{$shorts[0:MAX\_CLUSTERING]$, $worker\_id$}
            \State $shorts \gets shorts[MAX\_CLUSTERING:]$
        \EndWhile
        
        \Statex
        \State \Comment{4) group remaining longs (half-size)}
        \State $half \gets \max(1, \lfloor MAX\_CLUSTERING / 2 \rfloor)$
        \While{$longs \neq \emptyset$}
            \State $worker\_id \gets$ \Call{NewWorkerId}{}
            \State \Call{AssignWorker}{$longs[0:half]$, $worker\_id$}
            \State $longs \gets longs[half:]$
        \EndWhile
    \EndFunction
    \end{algorithmic}
\end{algorithm}

After Worker ID assignment, the \textbf{Non-Uniform} planner runs an additional algorithm, shown in Algorithm \ref{alg:resource_downgrading}, that attempts to downgrade resources of workers \textit{outside the critical path} as much as possible without introducing a new critical path, by iteratively simulating the global effect of downgrading resources of workers \textit{outside the critical path} with different configurations.

\begin{algorithm}
    \caption{Resource Downgrading Algorithm (used by Non-Uniform planner)}
    \label{alg:resource_downgrading}
    \small
    \begin{algorithmic}[1]
    \Require $dag$, $nodes$, $critical\_path\_ids$, $original\_cp\_time$, $configs$
    \State $workers\_outside \gets \emptyset$
    
    \Statex
    \State \Comment{1) Identify workers outside the critical path}
    \ForAll{$n \in nodes$} \Comment{nodes are topologically sorted}
        \State $wid \gets n.worker\_id$
        \If{$n.id \notin critical\_path\_ids \land \forall cp \in dag.cp\_nodes: wid \neq cp.worker\_id$}
            \State $workers\_outside \gets workers\_outside \cup \{wid\}$
        \EndIf
    \EndFor
    
    \State $nodes\_outside\_cp \gets \{n \in nodes \mid n.id \notin critical\_path\_ids\}$
    
    \Statex
    \State \Comment{2) Attempt downgrade for each worker outside critical path}
    \ForAll{$wid \in workers\_outside$}
        \State $last\_acceptable_\_rc \gets \{n.id: n.config \mid n \in nodes\_outside\_cp \land n.worker\_id = wid\}$
        
        \Statex
        \State \Comment{Iterate through weaker configurations (skip best)}
        \For{$i \gets 1$ \textbf{to} $|configs| - 1$}
            \State $trial \gets configs[i].\textsc{Clone}(wid)$
            
            \Statex
            \State \Comment{Apply trial configuration to all nodes of this worker}
            \ForAll{$n \in nodes\_outside\_cp$}
                \If{$n.worker\_id = wid$}
                    \State $n.config \gets trial$
                \EndIf
            \EndFor
            
            \Statex
            \State \Comment{Recompute workflow timing with predictions}
            \State $cp\_time \gets$ \Call{SimulateCriticalPathTime}{$dag$}
            
            \Statex
            \If{$cp\_time = original\_cp\_time$}
                \State \Comment{Good Downgrade}
                \ForAll{$n \in nodes\_outside\_cp$}
                    \If{$n.worker\_id = wid$}
                        \State $last\_acceptable\_rc[n.id] \gets n.config$
                    \EndIf
                \EndFor
            \Else
                \State \Comment{Bad Downgrade, revert}
                \ForAll{$n \in nodes\_outside\_cp$}
                    \If{$n.worker\_id = wid$}
                        \State $n.config \gets last\_acceptable\_rc[n.id]$
                    \EndIf
                \EndFor
                \State \textbf{break} \Comment{move to next worker}
            \EndIf
        \EndFor
    \EndFor
\end{algorithmic}
\end{algorithm}

With the information they have access to, planners can estimate whether it is worthwhile to offload a task to a more powerful worker. This involves weighing the overhead of uploading the input data, waiting for the worker to be provisioned, downloading dependencies, and then executing the task, against the alternative of simply executing the task on the current, less powerful worker.

Aside from their \texttt{worker\_id} and resource assignments, planners can also apply different \textbf{optimizations} to further improve the workflow execution. The optimizations to be used are selected by the user, as shown in Listing \ref{lst:setup_and_launch_workflow_execution}. Similarly to planners, we provide two optimizations: \textbf{pre-warm} and \textbf{pre-load}, but it is also possible to create new optimizations and define how workers should react to them. Now, we will describe the two base optimizations and how they are assigned to tasks:

\begin{enumerate}
\item\textbf{pre-warm}(worker\_config, delay\_s) [Pre-warming Workers]:
\begin{itemize}
\item \textit{Interpretation}: Tasks/Nodes with this optimization should perform a special invocation to the FaaS gateway, forcing it to launch a new worker with the specified resource configuration \texttt{worker\_config}. This can be used to warm up workers ahead of time and mask cold start latencies.
\item \textit{Assignment Logic}: For workers that are predicted to experience a cold start, the algorithm identifies an optimal worker to perform the pre-warming action. It searches for a task whose execution window aligns with a calculated pre-warming interval. This process balances two objectives: ensuring the pre-warmed worker does not go cold before it is required, while also not being warmed too late.
\item \textit{Integration with Task Handling Logic}: The optimization is bound to the task assigned to the optimal worker and includes a \texttt{delay\_s} parameter. Once the worker starts executing, it spawns a concurrent Python coroutine that waits for the specified delay before sending a special "empty invocation" to the FaaS gateway. This invocation triggers the initialization of a worker with the target resource configuration \texttt{worker\_config}.
\end{itemize}

\item\textbf{pre-load} [Pre-Loading Dependencies]:
\begin{itemize}
\item \textit{Interpretation}: Workers assigned to a task with this optimization will proactively download the task's dependencies as soon as they become available. This is achieved by subscribing to completion notifications from the \textit{Metadata Storage} for the task's upstream dependencies. When an upstream task finishes, the worker is notified and can immediately start downloading the result in the background, parallel to other ongoing computations. This strategy aims to reduce data-fetching latency when the task is finally ready to execute, as its inputs may already be present locally;
\item \textit{Assignment Logic}: The optimization is applied using a simple, single-pass heuristic. The logic iterates through the tasks and assigns the \texttt{pre-load} optimization to tasks that depend on at least two upstream tasks that are assigned to a different worker;
\item \textit{Integration with Task Handling Logic}: When a task becomes ready, before a worker starts fetching its dependencies, it prevents any new \texttt{pre-loads} from starting, and gathers a list of all ongoing \texttt{pre-loads}. It then fetches all other dependencies from storage and waits for all preloads to complete before executing the task.
\end{itemize}
\end{enumerate}

Because planners may sometimes lack sufficient information to make optimal decisions about optimization assignments, it is important to not only allow the user to select optimizations at the workflow-level, but also allow them the flexibility to specify optimizations at the \textit{task-level}. An example of this feature is shown in Listing~\ref{lst:dag_lang_example}, where the user explicitly requests that \texttt{task\_b} applies the \textit{pre-load} optimization.

Once optimizations are assigned, workflow planning is complete, and workers can begin execution. Because planning occurs on the user's machine (i.e., the machine launching the workflow), it is responsible for initiating the workflow by starting the initial workers. From that point onward, workers dynamically invoke additional workers as needed, following a choreographed, decentralized execution model.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/solution_workflowinstanceexample.png}
    \caption{Planned Workflow Execution Example}
    \label{fig:planned_workflow_execution_example}
\end{figure*}

To illustrate this execution model, Figure~\ref{fig:planned_workflow_execution_example} provides a visual trace of how a planned workflow would be executed. The diagram depicts the workflow with the optimizations and \texttt{worker\_id} assignments for each task. The non-dashed arrows represent task dependencies, while the dashed arrows represent interactions with the \textit{Intermediate Storage} to either upload or download task data. We can see that task outputs are only uploaded to storage when there is at least one downstream task that depends on it and is assigned to another worker. 

It is also worth noting that the planner assigned \texttt{Task 6} to \texttt{Worker 2}. This decision might be due to \texttt{Worker 2} being more powerful than \texttt{Worker 1}, and because the output of \texttt{Task 5} is larger than that of \texttt{Tasks 4} and \texttt{5}. Therefore, even if the task were executed on a more powerful worker (such as \texttt{Worker 3}, which handled \texttt{Task 3}), the potential performance gain would not offset the additional time or resources required. This is an example of a planner deciding to co-locate \texttt{Tasks 5 and 6} on the same worker to reduce data movement.

Regarding \textbf{optimizations}, we can see \texttt{Task 1} \texttt{pre-warming} \texttt{Worker 3}, by making a dummy invocation to the FaaS gateway, in an attempt to make \texttt{Task 3}'s worker have a warm start. The \texttt{pre-load} optimization is used in \texttt{Task 5}, where the planner decided that \texttt{Worker 2} should start downloading the external dependencies for \texttt{Task 6} (\texttt{Task 3} and \texttt{Task 4}) as soon as they are available. This \texttt{pre-loading} can begin as soon as \texttt{Task 6}'s dependencies are ready in storage, potentially overlapping with \texttt{Task 2}'s execution instead of \texttt{Task 5}, as shown in the figure.

\subsection{Decentralized Scheduling}

Since our target execution platform is FaaS, the worker logic is implemented as a FaaS handler. Due to the decentralized nature of our solution, workers will be responsible for performing both task execution and scheduling in a choreographed manner. 

When invoked, a worker receives the workflow structure with the embedded plan and the \texttt{task\_ids} of the tasks it should execute first. Rather than immediately executing the initial tasks, the worker first subscribes to \texttt{TASK\_READY} and \texttt{TASK\_COMPLETED} events for specific tasks. These events are essential both for enabling certain optimizations and for ensuring the worker follows the workflow plan correctly.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/choreographed_execution_example.png}
    \caption{Choreographed Scheduling Example}
    \label{fig:choreographed_scheduling_example}
\end{figure*}

After this preparation phase, the worker starts executing its initial tasks concurrently. The logic for executing tasks is the following:
\begin{enumerate}
    \item \textbf{Gathering Dependencies}: Check which dependencies are missing (not downloaded yet) and download them from storage. Some dependencies might be present in the local representation of the DAG if \texttt{pre-load} was applied, if the same worker executed some of the upstream tasks, or if the worker already downloaded large hardcoded data because previous tasks needed it;
    \item \textbf{Executing Task}: After gathering all task dependencies (upstream task outputs and hardcoded data), it executes the task's code. Its function code is embedded within the workflow representation so it can be easily executed by the workers, similarly to WUKONG. This enables the worker to remain generic, capable of receiving and executing arbitrary task code (except async functions, since \textit{cloudpickle} can't serialize them). Tasks' code execute on a separate thread, to avoid slowing down or even blocking (e.g., if the task code calls \texttt{time.sleep()}) the main thread, where the other coroutines are running: both task branch execution and other internal coroutines (i.e., Redis pub/sub listener, and delayed \texttt{pre-warm} requests);
    \item \textbf{Handling Output}: This phase is responsible for evaluating whether it's necessary to upload the task's output to storage and emitting a \texttt{TASK\_COMPLETED} event. A tasks' output is uploaded if there is at least one downstream task assigned to a different worker, or if it's the sink/last task of the workflow;
    \item \textbf{Updating Dependency Counters}: For each downstream task, the worker performs an \textit{atomic increment and get} operation on a \textit{"Dependency Counter"} (inspired by WUKONG~\cite{wukong_2}) stored in \textit{Metadata Storage}, which tracks how many dependencies of a task have been satisfied. If the counter value is the same as the number of dependencies for a downstream task, the worker emits a \texttt{TASK\_READY} event for that task, signaling other workers or workers that aren't active yet that the task became ready to execute;
    \item \textbf{Delegating Downstream Tasks}: After updating dependency counters, the worker knows which tasks became \textit{ready}. The worker then consults the execution plan to determine how to proceed for each of those tasks: for tasks assigned to the same worker and with no remaining unfulfilled dependencies, the worker will execute it (one \textit{coroutine} for each task); for tasks assigned to another worker \textbf{that is already active}, a \texttt{TASK\_READY} event is emitted; for tasks assigned to another worker \textbf{that is not active}, this worker launches the target worker, indicating the branches (\textit{immediate task identifiers}) it should execute. Because of the planner validation mentioned in Section ~\ref{ss:static_workflow_planning}, it is possible for a worker to know, at any point in time, whether another worker is already active or not just by looking at the workflow structure and respective plan.
\end{enumerate}

Both \textit{Intermediate Storage} and \textit{Metadata Storage} are implemented in Redis for deployment simplicity. For exchanging events among workers, we use Redis's Pub/Sub~\footnote{\url{https://redis.io/glossary/pub-sub/}}. Such events include \texttt{TASK\_READY} and \texttt{TASK\_COMPLETED}, and are implemented as Pub/Sub channels. 

We will now go over a practical example of how we use these events are used to coordinate decentralized scheduling. Figure~\ref{fig:choreographed_scheduling_example} presents an example of a workflow with four workers (A, B, C, and D) and eight tasks (T1-T8).

In this example, tasks complete in the following order: T1, T2, T3, T4, T5, T6, T7, and T8. The text in the arrows represent the order of actions performed by the worker who executed the task from which the arrow starts, where each action is separated by a semicolon. When T1 finishes, its worker will increment the dependency counter of T5 because it knows, by looking at the workflow representation, that T5 depends on other tasks from another worker. This storage operation returns "1", but T5 has 2 dependencies (T1 and T2) and as such, it isn't READY to execute. Then T1's worker sees T4 and since it's assigned to it, and it doesn't have any other dependencies, it schedules it for execution locally. Lastly T1 will see that T3 doesn't depend on any other tasks, meaning it is also ready for execution and, since it is assigned to another worker (Worker C), it sends a request to the FaaS gateway to launch Worker C, providing the branches it should execute.

Then, when T3 finishes execution, its worker will see that T7 depends on external tasks, so it updates the dependency counter of T7 and remains idle waiting for the \texttt{TASK\_READY} event for T7. Later, T4 finishes, realizes that T7 is meant to execute on another worker, increments its dependency counter, realizes all dependencies are met, and emits a \texttt{TASK\_READY} event for T7, to which Worker C will react to by start executing T7. Once T5 finishes execution, it realizes that T6 depends on external tasks, increments its dependency counter, realizes all dependencies are met, and since it's assigned to itself, it schedules it for execution locally.

Finally, T6 finishes, increments the dependency counter of T8 (because it has other dependencies), but since it's still missing 1 dependency, it exits. Later, then T7 finishes, it increments the dependency counter of T8 and realizes all dependencies are met and that the worker assigned to T8 (Worker D) isn't active yet, so it makes a request to the FaaS gateway to invoke this worker. Once T8 finishes, it emits a \texttt{TASK\_COMPLETED} event which is received by the client, who then downloads the result from \textit{Intermediate Storage}.

A workflow is considered complete once the output of the final (sink) task is available in storage. The worker that uploads this final result is also responsible for cleaning up all intermediate results before shutting down. This worker emits a \texttt{TASK\_COMPLETED} event for the sink task, triggering the client to retrieve the final result from \textit{Intermediate Storage}.

In contrast to traditional FaaS-based workflow engines that rely on a centralized scheduler, our system's scheduling is driven by the workers themselves. This decentralized model eliminates continuous coordination with a central controller, reducing overhead and removing a single point of failure. By enabling workers to trigger subsequent tasks immediately after completing their own, this approach minimizes scheduling latency and improves scalability, which primarily depends on the horizontal scalability of the underlying FaaS and storage layers. The client launches the initial set of workers, after which execution proceeds autonomously based on metadata embedded within the workflow representation passed among the workers. A lightweight coordination mechanism (atomic dependency counter) ensures that all tasks are eventually executed according to the scheduling plan and applied optimizations.

Having described the design and implementation of the system, we now turn to its evaluation. The next section presents the experimental setup, results, and analysis used to assess the strengths and weaknesses of our approach comparing it against a state-of-the-art FaaS workflow engine, WUKONG~\cite{wukong_2}.

% ------------------------------------------------ %

\section{Evaluation}
\label{s:evaluation}

% #############################################################################
\subsection{FaaS Environment Emulation}
\label{ss:faas_emulator}

To enable reproducible and controlled experiments, a lightweight Function-as-a-Service (FaaS) emulator was implemented in approximately \textasciitilde300 lines of Python. It reproduces the core behavior of serverless platforms while remaining simple, with greater observability, and allowing us to run lots of experiments inexpensively.

The system consists of two components: a \textbf{gateway service} and a \textbf{worker runtime}. The gateway, implemented as an HTTP server, receives invocation requests, manages container lifecycles, and enforces resource constraints using Docker's built-in CPU and memory limits\footnote{\url{https://docs.docker.com/engine/containers/resource_constraints/}}. Workers are packaged as Docker images containing the execution logic and a persistent background process to keep the container alive between invocations, until the gateway decides to shut it down (when idle for too long).

Function execution requests are issued to the gateway's \texttt{/job} endpoint, specifying task identifiers, resource configurations, and cached results. If no idle container with the required configuration is available and the maximum concurrency (32 containers) is reached, requests are queued until resources become free. To avoid resource contention, each container executes a single task at a time. This constraint is enforced through a file-based locking mechanism.

Idle containers are automatically removed after 7~seconds of inactivity, to simulate cold starts. The gateway also provides a \texttt{/warmup} endpoint that pre-allocates containers without executing worker logic, a simplification that makes it easier to perform \textit{pre-warming} without adding extra logic to the workers code. To improve observability, worker logs (stdout and stderr) are streamed to the gateway and captured in real time for debugging.

\subsection{Research Questions}

With the evaluation of our work, we aim to address the following research questions:

\begin{itemize}
\item \textbf{RQ1:} To what extent can historical metrics from previous workflow executions improve the accuracy of predicting serverless workflow behavior, specifically regarding execution time, container startup latency, data transfer performance, and function I/O characteristics?

\item \textbf{RQ2:} What are the main advantages and limitations of applying prediction-based scheduling strategies in serverless workflow environments, compared to traditional reactive or static scheduling approaches? Can the proposed system achieve a lower \textit{makespan} and reduced overall resource consumption compared to WUKONG~\cite{wukong_2}, a decentralized serverless workflow engine that employs its own set of data-locality optimizations?

\item \textbf{RQ3:} How effective are the proposed optimizations in practice? In particular, how much does \texttt{pre-load} contribute to hiding latency and enabling earlier task execution, and how beneficial is \texttt{pre-warming} in reducing cold-start delays and improving overall workflow performance?

\item \textbf{RQ4:} How much performance improvement can be achieved by adopting a non-uniform worker resource allocation strategy, compared to a uniform scheduling approach that assigns identical resource configurations to all tasks? And what is the trade-off between the achieved performance gains and the additional resource consumption introduced by this adaptive allocation strategy?
\end{itemize}

To help answer these questions we had to collect a wide range of metrics:

\begin{itemize}
    \item For each \textbf{task}:
    \begin{itemize}
        \item Timestamp of when it started being handled (before executing its tasks);
        \item Time to download all dependencies;
        \item Size and time to download each of the dependencies;
        \item Task execution time;
        \item Size and time to upload output;
        \item Optimization-specific metrics of all optimizations applied to a given task on a particular workflow instance.
    \end{itemize}
    \item For each \textbf{workflow instance}:
    \begin{itemize}
        \item Entire workflow plan, with worker resource and ID assignments, as well as optimizations;
        \item Time at which user submitted the workflow;
        \item Monitor Docker containers during the workflow, recording total run-time and memory allocation in GB-seconds, similar to AWS Lambda's cost calculation, which is based on memory (GB) * run-time (seconds).
    \end{itemize}
    \item For each \textbf{worker}:
    \begin{itemize}
        \item Resource Configuration (CPUs and Memory);
        \item Time at which a worker invocation was made to the FaaS Gateway and time at which the worker script started executing (used to calculate worker startup latency);
        \item Worker state, indicating whether the worker script was executed in a \textit{cold} or \textit{warm} environment. A cold start occurs when a new container must be created to run the worker, while a warm start reuses an existing, previously initialized container. The system determines this state using a file-based locking mechanism: during startup, the worker attempts an atomic \textit{“create-if-not-exists”} operation on the file \texttt{/tmp/worker\_startup.atomic}. If the file already exists, it implies that the container has been used before, and the current execution is therefore a warm start; otherwise, it is a cold start. When a container is \textit{pre-warmed}, this file is proactively created during initialization, even though the worker script itself is not yet executed.
    \end{itemize}
\end{itemize}

In the following section, we describe the experimental setup used to evaluate the proposed system.

\subsection{Experimental Setup}

To evaluate our proposed solution, we deployed the FaaS emulator described in Section~\ref{ss:faas_emulator} on an AMD EPYC 7282 16-Core Processor with 125GiB of RAM, running Ubuntu 22.04.5 LTS. Both \textit{Metadata Storage} and \textit{Intermediate Storage} were deployed as Redis Docker containers. The client, responsible for submitting the workflow and uploading hardcoded dependencies, was also run on the same machine. We added some artificial delay in both storage and gateway interactions to try simulating a more realistic environment. This delay is applied before requests are made, and the value we used was 30ms of round-trip time. This value was chosen based on observations of median latency between AWS Europe data centers in the same region being around 8ms and around 15ms across different data centers~\footnote{\url{https://www.cloudping.co/}}.

The workflows used to test our system were the following:

\begin{itemize}
    \item \textbf{Matrix Multiplication}: A workflow with a straightforward structure that performs distributed matrix multiplication, comparable to a workflow used in WUKONG's evaluation.
    \item \textbf{Tree Reduction}: A workflow that performs a hierarchical reduction over a list of numeric values, identical in structure to a workflow from WUKONG's evaluation.
    \item \textbf{Text Analysis}: A workflow with a more complex structure, designed to simulate a multi-stage text processing pipeline on a large text file (750,000 lines).
    \item \textbf{Image Transformation}: A complex, highly parallel workflow composed of 130 tasks that applies multiple transformations to an input image, featuring large fan-outs and heterogeneous task execution times.
\end{itemize}

We ran our experiments with three different SLAs: 50th percentile (median), 75th percentile, and 90th percentile, and analyzed the fulfillment success rates of each SLA, as well as their prediction accuracy.

To assess our research questions, we implemented and evaluated three core planners: our proposed \textbf{Uniform} and \textbf{Non-Uniform} planners, and a planner replicating \textbf{WUKONG's} scheduling model. For a comprehensive comparison, we tested each planner with and without their respective optimizations enabled (\textit{pre-load/pre-warm} for our planners, and \textit{Task Clustering/Delayed I/O} for WUKONG). Planners using a uniform resource strategy (\textit{Uniform} and \textit{WUKONG} variations) assigned workers with 2GB of memory. The \textit{Non-Uniform} planner could assign more powerful workers from a predefined list of configurations (\textit{2GB, 4GB, and 8GB}). In total, we evaluated six planner variations.

The experimental process was automated with a script that iterated through every combination of planner variation, SLA, and workflow. Each unique configuration was executed 10 times to account for performance variability, resulting in 720 total experiments for analysis. Before each run, the environment was reset to ensure that the initial tasks of every workflow would trigger a cold start, ensuring consistency.

\section{Results}

This section summarizes the main evaluation results of our proposed planners, our \textbf{WUKONG} planner implementation, and their optimized variants, focusing on prediction accuracy, workflow execution time (\textit{makespan}), and resource efficiency. Note that the optimized versions of the \textbf{Uniform} and \textbf{Non-Uniform} planners incorporate our proposed \textit{pre-loading} and \textit{pre-warming} optimizations, while the optimized \textbf{WUKONG} variant uses its own native \textit{Task Clustering} and \textit{Delayed I/O} techniques.

\subsection{Prediction Accuracy and SLA Fulfillment}

\begin{figure}[H]
\centering
\includegraphics[width=1.15\columnwidth]{figures/eval/1_predictions/prediction_accuracy_by_sla_and_fullfilment.png}
\caption{Predictions accuracy across all planners and SLA fulfillment rates}
\label{fig:predictions_accuracy_by_sla_and_fullfilment}
\end{figure}

Figure~\ref{fig:predictions_accuracy_by_sla_and_fullfilment} shows the prediction accuracy and SLA fulfillment rates across all planners (excluding WUKONG, which does not employ predictions). Execution time predictions were highly accurate, with median relative errors below \textbf{9.3\%}, confirming the reliability of our predictive models for workflow planning on a semi-predictable environment. Data transfer time predictions were less precise, with errors around \textbf{35\%}, likely due to their higher variability and network-dependent characteristics. SLA fulfillment rates aligned with expectations: \textbf{41.9-71.5\%} for P50, \textbf{66.7-89.7\%} for P75, and \textbf{86.9-100\%} for P90.

\subsection{Execution Performance}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/eval/2_metrics/makespan dist.png}
\caption{\textit{Makespan} distribution across all planners}
\label{fig:makespan_dist}
\end{figure}

As shown in Figure~\ref{fig:makespan_dist}, all versions of our planners outperformed both \textbf{WUKONG} implementations in terms of makespan. The non-optimized \textbf{Uniform} planner was \textbf{12.6\% faster} than the optimized version of \textbf{WUKONG}, while the optimized \textbf{Non-Uniform} planner achieved the best overall performance.

The observed gap between optimized and non-optimized versions of the \textbf{Non-Uniform} planner further confirms that our proposed optimizations (\textit{pre-loading} and \textit{pre-warming}) contribute directly to faster task execution and reduced startup overhead. The same improvement was not observed between the baseline and optimized \textbf{Uniform} planners. We attribute this to \textbf{resource contention}: the \textbf{Uniform} planner allocates fewer resources per worker, and the \textit{pre-loading} optimization increases concurrency within a worker, which can inadvertently slow overall execution.

\subsection{Optimization Effects}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/eval/3_opts/time breakdown.png}
\caption{Time breakdown analysis}
\label{fig:time_breakdown_analysis}
\end{figure}

Figure~\ref{fig:time_breakdown_analysis} highlights the effect of both our scheduling approach that tries to cluster more tasks per worker and our optimizations (\textit{pre-loading} and \textit{pre-warming}). Both optimized planners significantly reduced \textbf{worker startup} and \textbf{dependency waiting} times compared to their base versions and to \textbf{WUKONG}. The use of \textit{pre-warming} minimized cold-start penalties, saving between \textbf{2-6 seconds} on average, while \textit{pre-loading} allowed tasks to start executing earlier by proactively transferring dependencies.

The optimized \textbf{Non-Uniform} planner achieved the shortest total execution time, whereas the optimized \textbf{Uniform} planner maintained higher resource efficiency. This indicates that the system can prioritize either speed or efficiency depending on the selected planner.

\subsection{Performance vs. Resource Efficiency}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/eval/4_res_usage/makespan compare.png}
\caption{\textit{Makespan} comparison}
\label{fig:makespan_comparison}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/eval/4_res_usage/resources compare.png}
\caption{Resource usage comparison}
\label{fig:resources_comparison}
\end{figure}

For context, \textbf{GB-seconds} is a commonly used billing metric in serverless platforms, including AWS Lambda~\footnote{\url{https://aws.amazon.com/lambda/pricing/}}, calculated as the product of allocated memory (GB) and execution duration (seconds). Figures~\ref{fig:makespan_comparison} and~\ref{fig:resources_comparison} summarize the trade-off between execution speed and resource consumption across all tested planners. The optimized \textbf{Non-Uniform} planner achieved a \textbf{53\% faster makespan} and consumed \textbf{45\% less memory} than its non-optimized counterpart, making it the best performer overall. While the optimized Uniform planner is more resource-efficient, consuming \textbf{34.9\%} fewer GB-seconds than its optimized Non-Uniform counterpart, it is also \textbf{152\% slower}.

Comparing to \textbf{WUKONG}, both versions were less efficient, with the optimized version only \textbf{2.7\% faster} and \textbf{25\% less resource-intensive} than its baseline, but still \textbf{14.4\% slower} and consuming \textbf{56.2\%} more resources than our \textbf{Uniform} planner.

\subsection{Summary}

Overall, our results conclusively demonstrate that the proposed \textbf{Uniform} and \textbf{Non-Uniform} planners outperform the \textbf{WUKONG} model in both execution speed and resource efficiency.

The \textbf{Uniform} planner establishes itself as the most cost-effective approach. It not only achieves the lowest median resource usage (in GB-seconds) among all tested configurations but also executes workflows faster than both the baseline and optimized versions of \textbf{WUKONG}. This highlights a key weakness in \textbf{WUKONG}'s scheduling strategy, where its handling of fan-outs leads to excessive worker startup and synchronization overhead, inflating both runtime and resource consumption in most scenarios.

For use cases prioritizing maximum speed, the \textbf{Non-Uniform} planner provides a distinct advantage over the \textbf{Uniform} planner, consistently delivering the shortest makespan. This performance gain is achieved by leveraging more powerful workers, presenting a clear trade-off between execution time and cost. The impact of our optimizations is most evident with this planner; its optimized version achieves a makespan over \textbf{50\%} shorter while consuming \textbf{45\%} fewer resources.

The \textbf{SLA} parameter, specified as a \textbf{percentile}, was also validated as a practical parameter for managing user expectations. Our lightweight, SLA-driven predictions provided reliable results without the computational overhead of complex models, often used in other solutions~\cite{cose,orion,jolteon,predictiveadaptive}, enabling a path toward future dynamic scheduling.

In summary, our results validate three key findings. First, that predictive optimizations like \textit{pre-loading} and \textit{pre-warming} are critical for mitigating latency and inefficiency in serverless workflows. Second, it presents a practical choice for developers/data scientists: the \textbf{Uniform} planner for robust and resource-efficient execution, and the \textbf{Non-Uniform} planner for greater performance in latency-sensitive applications. Third, it shows the importance and positive impact of properly \textbf{balancing resource contention and data locality} in serverless workflows.

% ------------------------------------------------ %

\section{Related Work}
\label{s:related_work}

This section highlights the most relevant related systems to our work, tracing the evolution from traditional cluster-based frameworks to modern cloud-native workflow orchestration platforms and serverless workflow engines. We highlight their key design principles, limitations, and the innovations that inspired our approach.

\subsection{Traditional Workflow Scheduling}
Traditionally, executing computation workflows has relied on distributed data processing frameworks designed to manage computation and data across clusters of machines. Frameworks such as Hadoop~\cite{apache_hadoop}, Apache Spark~\cite{apache_spark}, and Apache Flink~\cite{apache_flink} provide abstractions for parallel and distributed execution, enabling efficient coordination, scheduling, and data movement across clusters. A key programming model that influenced these systems is MapReduce~\cite{mapreduce}, which allows developers to process massive datasets by implementing simple \texttt{map} and \texttt{reduce} operations, with canonical example jobs like WordCount. Spark and Flink extend this model with in-memory processing and stream-oriented computation. More recently, Dask~\cite{dask_python} has emerged as a flexible Python-based framework that enables parallel and distributed execution of complex task graphs. Unlike traditional MapReduce-style frameworks, Dask supports a more generic programming model that goes beyond simple map and reduce operations, allowing execution of heterogeneous tasks across clusters while maintaining ease of integration with Python libraries. Dask is particularly well-suited for data science applications due to its seamless integration with NumPy, Pandas, and other libraries, and its ability to handle large-scale mathematical computations efficiently and in a distributed manner, through Dask Distributed~\cite{dask_python_distributed}.

Collectively, these frameworks rely on clusters of machines to distribute computation and manage data, providing scalability and parallelism for large-scale workflows. They offer several advantages, including control over the underlying system, easier to achieve better data locality, efficient use of dedicated resources, and the flexibility to fine-tune scheduling and execution policies. These features make them well-suited for complex, resource-intensive workloads where predictable performance is critical. However, the cluster-based approach also introduces overheads and has inherent limitations related to ease-of-use, deployment, resource provisioning, and slow scaling of resources, which can limit performance and efficiency, particularly for dynamic and embarrassingly parallel workflows.

\subsection{Cloud-Native Workflow Scheduling}
Traditional cluster-based frameworks often require significant expertise to configure, deploy, and maintain. Cloud-native workflow solutions provide a higher-level, managed approach, enabling developers to orchestrate workflows without worrying as much about underlying infrastructure. These platforms simplify deployment, and automatically handle resource allocation, fault tolerance, and state management.

Prominent commercial serverless workflow platforms, also referred to as \textbf{stateful functions}, include AWS Step Functions~\cite{aws_step_functions}, Azure Durable Functions~\cite{azure_durable_functions}, and Google Cloud Workflows~\cite{google_cloud_workflows}. These solutions allow users to create workflows by composing \textit{stateless} functions together. The platform is then responsible for the orchestration part, managing workflow state, intermediate results, and fault tolerance without requiring developers to implement state management themselves. These platforms typically employ \textit{checkpointing} techniques to persist workflow state, allowing the orchestrator or stateful function to pause while waiting for stages of the workflow to complete and then resume execution to trigger subsequent stages. Checkpointing also ensures that workflow execution can recover correctly after failures.

This orchestration and management capability is typically billed separately, reflecting both the convenience and reliability it provides and the additional resources required to maintain workflow state and fault tolerance. The main differences among these platforms lie in their execution model, workflow definition, and target use cases. Both AWS Step Functions and Google Cloud Workflows use JSON to represent workflow state machines, while Azure Durable Functions provides a more flexible programming model, where workflows are defined in code. One advantage of using such commercial services is that integration with other cloud services is easier, as they are tightly integrated with the AWS, Azure, and Google Cloud ecosystems.

In addition, there are also open-source workflow orchestration projects that run on general-purpose cloud or on-premises infrastructure, combatting vendor lock-in by providing better interoperability and flexibility. Apache Airflow~\cite{apache_airflow} is a widely used example, allowing workflows to easily be defined in Python code. Airflow has a rich ecosystem of pre-built integrations for databases, message queues, and major cloud services, simplifying the connection of workflows to diverse external services. It is mostly used for ETL ~\footnote{\label{fn:etl}https://pt.wikipedia.org/wiki/Extract,\_transform,\_load} processes, machine learning workflows, and batch data processing.

\subsection{FaaS Runtime Extensions}
While commercial and open-source workflow platforms simplify orchestration and state management, they generally operate on top of standard FaaS runtimes and cannot fully address the inefficiencies inherent in serverless platforms, such as cold starts, limited inter-function communication, and lack of data locality. To address these limitations, a significant body of research has focused on modifying the underlying FaaS runtime or proposing extensions that improve data locality and scheduling efficiency. Palette~\cite{palette_load_balancing}, Faa\$T~\cite{faast_caching}, and Lambdata~\cite{lambdata_intents} are three prominent examples of such extensions.

\subsubsection{Palette Load Balancing}
Palette Load Balancing improves data locality by introducing the concept of \textit{colors} as \textit{locality hints}. These colors are parameters attached to function invocations, allowing users to express which invocations should be co-located on the same worker. Palette then attempts to route invocations with the same color to the same instance, enabling subsequent invocations to access locally cached data rather than fetching it from remote storage. Colors are treated as \textit{hints}, so the system can ignore them if resource constraints require it. This approach improves cache hit ratios, reduces data transfer costs, and gives users and systems enough flexibility to define coloring policies using different strategies.

\subsubsection{Faa\$T}
Faa\$T provides a transparent, auto-scaling distributed cache for serverless functions. Each application has its own dedicated in-memory cache, called a \textit{cachelet}, which stores data accessed during function executions and makes it available for subsequent calls. Faa\$T's transparency is achieved by operating without requiring changes to application code, as it intercepts data requests and checks the cache before accessing remote storage. \textit{Cachelets} collaborate to form a distributed cache using consistent hashing, dynamically scaling up or down based on application workload.

\subsubsection{Lambdata}
Lambdata focuses on optimizing data locality by using \textit{user-provided data intents}. Developers explicitly annotate functions with \textit{get\_data} and \textit{put\_data} parameters, specifying which data objects the function will read or write. Lambdata then uses these annotations to make informed scheduling decisions such as co-locating functions that share data and leveraging local caches to minimize remote data transfers. This approach simplifies deployment and integration, as it does not require a distributed cache or complex runtime modifications, but relies on accurate intent declarations to achieve effective data reuse.

Together, these FaaS runtime extensions illustrate different strategies for improving data locality and execution efficiency in serverless environments.

\subsection{Serverless Workflow Scheduling Execution Engines}

Several frameworks have been designed to execute workflows efficiently on \textbf{unmodified} FaaS infrastructure, each offering distinct approaches to address the challenges of stateless, distributed computing. These solutions represent significant innovations in workflow orchestration, with varying trade-offs in terms of scalability, data locality, architectural complexity and ease-of-use and inspired our work. Here we will mention the most relevant workflow execution engines built to run on top of FaaS.

\subsubsection{DEWE v3}
DEWE v3~\cite{dewe_v3} introduces an innovative hybrid approach to serverless workflow orchestration that combines the best aspects of both serverless and serverful computing models. This hybrid workflow execution engine intelligently distributes tasks based on their characteristics: \textit{short tasks} are run on FaaS workers while \textit{longer tasks} run on virtual machines. The system employs a queue-based job distribution mechanism where jobs expected to complete within FaaS limits are published to a common job queue for serverless execution, while long-running jobs are directed to a separate queue for local, serverful execution on dedicated servers. Jobs that fail to execute on FaaS workers, for being longer than expected and exceeding execution limits imposed by the platform, are redirected to the serverful queue. This dual-execution model enables DEWE to accommodate workflows with diverse resource consumption patterns. This system proves particularly effective for scientific workflows, such as Montage~\cite{montage_astronomy}, where task durations and resource requirements vary significantly. However, this hybrid approach introduces specific trade-offs. Latency-sensitive workflows may be slowed by job queuing overhead. In addition, hybrid deployments often lead to resource underutilization, as serverful workers may sit idle when most tasks are executed on FaaS. Finally, the centralized workflow manager can become a scalability bottleneck when handling many short tasks.

\subsubsection{PyWren}
PyWren~\cite{pywren}, representing one of the pioneering pure serverless approaches, demonstrates the potential and limitations of leveraging unmodified serverless infrastructure for distributed computation. Built atop AWS Lambda, PyWren focuses on executing arbitrary Python functions as stateless serverless functions with minimal user management overhead, automatically handling function execution, dependencies, S3 bucket storage for serialized code and intermediate data. The system is ideal for embarrassingly parallel workloads, also known as \textit{"bag-of-tasks"} scenarios, with many independent, parallel tasks such as simple data transformations, scientific simulations, parallel model training, and large-scale media processing. While PyWren’s serverless orchestration model provides excellent scalability and removes the burden of infrastructure management, its simplicity limits its applicability. It is not well-suited for workflows with complex dependency structures or those that require sharing large intermediate results through object storage. Moreover, latency-sensitive applications are disadvantaged by function cold starts, since PyWren does not include mechanisms to mitigate their impact.

\subsubsection{Unum}
Unum~\cite{unum_decentralized_orchestrator} takes a radically different approach from the two previous solutions by decentralizing orchestration logic entirely, eliminating the need for a standalone orchestrator service. This application-level serverless workflow orchestration system embeds orchestration logic directly into a library that wraps user-defined FaaS functions, leveraging an external scalable consistent data store for coordination during fan-ins and execution correctness. Unum introduces an Intermediate Representation (IR) to capture information about workflow progression (nearby tasks) and relies only on minimal, common serverless APIs (function invocation and basic data store operations) available across cloud platforms. This design choice provides exceptional portability and cost-effectiveness, as it can run on unmodified serverless infrastructure. Unum can also compile workflows defined in languages from providers like AWS Step Functions and Google Cloud Workflows into its IR format. However, its generic approach comes with trade-offs: it currently supports only statically defined control structures and cannot express workflows where the next step is determined dynamically at runtime, and it lacks data locality optimizations since it cannot force related tasks to execute on the same worker, with each function instance executing only its specific task before triggering the next function.

\subsubsection{WUKONG}
\label{prw:wukong}
WUKONG~\cite{wukong_2} represents the most sophisticated approach among these solutions, designed as a decentralized locality-enhanced serverless workflow engine. WUKONG addresses the limitations of traditional serverful models like Dask Distributed while maximizing the advantages of serverless computing, focusing on improving scale-out speed and enhancing data locality to minimize large object movement. The system's architecture is divided into static components (operating before workflow execution) and dynamic components (operating during execution). The static scheduler includes a \textbf{DAG Generator} that converts Python code into DAGs (using Dask), a \textbf{Schedule Generator} that creates \texttt{n} static schedules for \texttt{n} root/leaf nodes (each containing every reachable task in a depth-first search starting at that node), \textbf{Initial Task Executor Invokers} that launches the first Lambda instances for each root task, and a \textbf{Process} on the client that waits for and downloads final results. 

After receiving the initial schedules, FaaS workers (referred to as \textit{AWS Lambda Executors}) drive workflow execution. Workers execute tasks until they encounter a \textit{fan-out}, at which point they transfer data to intermediate storage, execute 1 of the \texttt{N} fan-out tasks and invoke \textit{N - 1} \textit{new} executors for the other tasks. Then, when they find a \textit{fan-in}, the group of executors that reach the common fan-in node cooperate using a \textit{dynamic scheduling} model to select only one executor to proceed. This coordination is managed through a shared \textbf{dependency counter} in a Key-Value Store (KVS). Each involved executor \textit{atomically} updates this counter; the one whose update satisfies the final input dependency for the fan-in task will execute that task and continue along its static schedule. The other executors transfer their intermediate data to storage, and then stop their execution, decreasing the workflow parallelism.

Besides dynamic scheduling, WUKONG employs data locality optimization techniques designed to avoid moving large data objects; \textbf{Task Clustering for Fan-Out Operations} allows executors to continue executing downstream tasks when a fan-out task produces large outputs, becoming the executor of multiple fan-out targets rather than just executing 1 and invoking \texttt{N - 1} new executors; \textbf{Task Clustering for Fan-In Operations} enables executors to recheck dependencies after uploading large objects to storage, potentially executing fan-in tasks themselves if dependencies are satisfied during the upload process, potentially avoiding large downloads; \textbf{Delayed I/O} allows executors to hold off on writing large intermediate results to external storage until it is absolutely necessary. Instead of immediately storing data when some downstream tasks are not yet ready, the executor first runs any tasks that can proceed and then checks again if the remaining ones have become ready. If they have, the executor can execute them directly using the data already in memory, avoiding both the write and a later read from storage. Only when no further progress is possible are the results finally written out. This can reduce unnecessary data transfers.

These optimizations, combined with WUKONG's decentralized scheduling approach, significantly enhance performance compared to both \textbf{Dask Distributed} and \textbf{PyWren} by minimizing data transfer overhead and eliminating central scheduler bottlenecks. However, WUKONG shares the limitation of supporting only statically defined control structures, requiring workflow DAGs to be known ahead of time, similarly to our proposed solution. Additionally, its optimization heuristics can lead to inefficiencies in certain scenarios: \textit{Delayed I/O} may increase makespan and storage usage if dependencies aren't met after retries; fan-in conflicts where multiple tasks produce large objects can result in resource waste depending on upload timing; and fan-out scenarios with small inputs may not justify the overhead of invoking multiple executors as it can make subsequent fan-in's more expensive. Furthermore, WUKONG assumes a homogeneous execution environment, where all workers provide identical resources (e.g., each task is allocated 2 CPU cores and 512 MB of memory), which prevents tailoring resources to tasks with different computational or memory demands.

While WUKONG represents a significant advance in serverless workflow orchestration through its decentralization, its scheduling and optimizations remain limited. The system bases decisions only on the next stage of the workflow (i.e., one-to-one, fan-in, or fan-out transitions). We refer to this as \textit{one-step scheduling}, since it relies solely on information about the next step. Crucially, WUKONG does not exploit the global knowledge of the workflow structure, even though the entire structure is known before execution begins. Consequently, its optimizations rely on fixed, heuristic-based strategies that enforce an extreme trade-off between data locality and resource contention. Optimizations like \textit{Task Clustering} and \textit{Delayed I/O} achieve \textbf{maximum data locality} by having executors hold large objects and prolong execution, but this comes at the cost of \textbf{high resource contention}. Conversely, disabling these optimizations results in \textbf{no resource contention}, leading to \textbf{no data locality}. This highlights the absence of adaptive mechanisms to achieve a \textit{dynamic balance between locality and resource utilization}, resulting in suboptimal performance when workload behavior deviates from expected patterns. With our solution we demonstrated that this dynamic balance is critical to improving scheduling efficiency.

\subsection{Discussion}
The existing body of related work highlights a clear progression in workflow orchestration strategies, moving from traditional cluster-based frameworks to cloud-native platforms and finally to serverless execution engines. Cluster-based systems such as Hadoop, Spark, Flink, and Dask emphasize fine-grained control, strong data locality, and predictable performance, but they incur significant operational overhead, limited elasticity, and often require technical expertise to deploy and manage. Commercial cloud-native platforms like AWS Step Functions and Azure Durable Functions simplify deployment and management by abstracting state handling and orchestration, but they typically incur additional costs and are bound to vendor-specific ecosystems. Runtime extensions such as Palette, Faa\$T, and Lambdata tackle the core inefficiencies of serverless platforms by enhancing data locality and reducing communication overhead.

Serverless workflow engines like PyWren, Unum, and WUKONG represent the most direct attempts to build high-performance workflow execution on unmodified FaaS infrastructure. While PyWren demonstrates the feasibility of large-scale embarrassingly parallel workloads, it struggles with complex workflows. Unum advances decentralization by embedding orchestration logic directly into functions, achieving portability and cost efficiency, but it remains limited in its support for dynamic control structures and lacks optimizations for data locality. WUKONG achieves major improvements through decentralization and data-aware heuristics such as \textit{Task Clustering} and \textit{Delayed I/O}, delivering great scalability and cost efficiency. Taken together, these systems provide the most direct inspiration for our work, while also highlighting key open challenges that our approach seeks to address.

% ------------------------------------------------ %

\section{Conclusion}

\subsection{Achievements}

This work explored a novel scheduling approach for serverless workflows, leveraging historical task metrics to \textbf{reduce makespan} and \textbf{improve resource efficiency}. We designed and evaluated a decentralized workflow engine integrating predictive planning with global knowledge of workflow structure.

Evaluation across multiple workflows showed that our planners consistently outperformed \textbf{WUKONG}. The optimized \textbf{Non-Uniform} planner achieved the shortest makespan (approximately \textbf{63\% faster} than optimized WUKONG) while the \textbf{Uniform} planner proved to be the most resource-efficient, consuming about \textbf{36\% fewer GB-seconds}. These improvements stem from effective task co-location strategies and the proposed \textbf{pre-warming} and \textbf{pre-loading} optimizations, which reduced both worker startup delays and data waiting times. 

The solution's modular architecture, separating \textbf{prediction}, \textbf{planning}, and \textbf{execution} layers, provides a solid foundation for future research and is available at \textit{https://github.com/diogojesusdev/octoflows}.

\subsection{Limitations and Future Work}

Limitations of our solution include unbounded historical data growth, potential conflicts in optimizations, and limited error handling. Future work could explore more aggressive strategies, such as \textit{task duplication}, support for dynamic fan-outs, richer user control, and interactive dashboards. Another promising direction lies in adapting to potential platform evolutions, such as FaaS platforms supporting \textbf{decoupled memory and CPU configurations}. Allowing these resources to be configured independently would enhance efficiency and enable \textbf{finer-grained pricing models}. While this introduces complexity in \textbf{scheduling, resource allocation, and locality optimization} for the provider, our solution would \textbf{naturally extend} to such environments, as its design is not constrained by specific resource coupling. Nevertheless, the increased dimensionality of possible resource configurations would significantly expand the prediction search space, potentially leading to slower planning times.

Finally, broader evaluation (comparing prediction strategies, deploying on commercial FaaS platforms, or even on edge computing environments~\cite{cluster-comp-19,comp-nets-22}, and testing complex real-world workflows) would further validate the approach and its practical applicability.

\bibliographystyle{IEEEtran}
\bibliography{references}

\clearpage
\onecolumn
\appendix

\begin{figure}[h]
   \centering
   \includegraphics[width=0.7\linewidth, height=0.7\textheight]{figures/dag_image_gemm.png}
   \caption{Matrix Multiplication Workflow}
   \label{fig:matrix_multiplication}
\end{figure}
\begin{figure}[h]
   \centering
   \includegraphics[width=\linewidth, height=0.85\textheight]{figures/dag_image_tree_reduction.png}
   \caption{Tree Reduction Workflow (Partial)}
   \label{fig:tree_reduction}
\end{figure}
\begin{figure}[h]
   \centering
   \includegraphics[width=\linewidth, height=0.35\textheight]{figures/dag_image_text_analysis.png}
   \caption{Text Analysis Workflow (Partial)}
   \label{fig:text_analysis}
\end{figure}
\begin{figure}[h]
   \centering
   \includegraphics[width=\linewidth, height=0.35\textheight]{figures/dag_image_image_transformer.png}
   \caption{Image Transformation Workflow (Partial)}
   \label{fig:image_transformer}
\end{figure}

\end{document}
